---
title: 딥러닝 기반 반려견 울음소리 감지와 감정 및 상태 분석(2)
author: nokh9503
date: 2020-12-13 00:00:00 +0800
categories: [AI, Deep Learning]
tags: [AI, Deep Learning]
---

## 3. 프로젝트 내용
### 3.1 제안 방법

![proposed method](/assets/img/ai/project/proposed_method.png)

&nbsp;&nbsp;추출한 음원파일을  `Mel-spectrogram`으로 변환하고 **frequency scale**, **time-step** 등과 같은 이미지 변환에 대한 **hyper parameter**를 initalize하여 이미지 파일로 만든다.

&nbsp;&nbsp;이후 `비지도 학습`을 이용하여 입력 값에 대한 label이 주어지지 않은 데이터들을 군집화를 통해 분류하고 특징의 채널 간의 dependency를 분석하고 중요도가 높은 채널에 더 높은 가중치를 부여하는 `attention`을 적용하여 mel-spectrogram 이미지의 가장 두드러진 특징을 강조한다. 또한 _모바일_ 이나 _임베디드 시스템_ 과 같은 환경에서도 성능을 유지할 수 있는 CNN 모델인 `Mobile-Net`을 이용하여 학습한다.  
  
### 3.2 데이터 추출
#### 3.2.1 Audacity

![audacity](/assets/img/ai/project/audacity.png)

- 대표적인 머신러닝 데이터 분석 플랫폼인 _Kaggle_ 과 동영상 플랫폼인 _YouTube_ 를 통해 개의 울음소리에 대한 음원을 `.wav` 파일로 만든다.
- YouTube에서 추출한 데이터는 위의 그림과 같이 감정 인식에 대한 불필요한 정보가 있는 _노이즈_ 와 _무음 구간_ 이 존재하기 때문에 Audacity 프로그램을 사용하여 최대한 **clean**한 파일로 만들어준다.
- Audacity 프로그램을 사용하면 spectrogram을 확인할 수 있으며, 음원 파일을 편집할 수 있다.

#### 3.2.2 오디오 편집

- 딥러닝 기반으로 소리를 이용한 학습 데이터는 정형화된 파일이어야 한다. `3.2.1.`에서 소개한 방법으로 노이즈와 무음 구간을 최대한 없앴지만 아직까지는 **raw**한 데이터이기 때문에 10초짜리 오디오 채널이 1개인 음원 데이터로 다시 가공했다.
- Audacity를 이용하여 데이터를 자르는 방법이 있지만 부정확하고 직접 잘라야 하는 단점이 있었다. 또한, 데이터가 크고 저장공간이 부족했기 때문에 `Google Colab`에서 간단한 `Python` 코드를 이용하여 가공된 데이터를 드라이브에 저장했다.

```python
from pydub import AudioSegment

def cutAudio(input_file, maxtime):
    t1 = 0
    t2 = t1 + 10    # 10초 단위로 끊음
    label_num = 0   # label 번호를 설정

    while t1 < maxtime:
        start = t1 * 1000
        end = t2 * 1000
        newAudio = AudioSegment.from_wav(input_file)
        newAudio = newAudio[start:end]
        newAudio = newAudio.set_channels(1)
        newAudio.export('path where new audio export' + str(label_num) + '.wav', format = "wav")        # 새로운 형태로 export

        label_num += 1
        t1 += 1
        t2 += 2

# main program
cutAudio("file name", time)
```

#### 3.2.3 Mel-Spectrogram 변환

- **주파수 성분**은 소리 데이터의 가장 특징적인 요소 중 하나이며 `Mel-Spectrogram`은 인간의 청각에 맞게 시간과 주파수 축이 **멜 스케일**로 변환되는 소리 데이터 이미지이다. 따라서 Mel-Spectrogram 이미지를 사용하면 소리에 대한 특징 및 클러스터를 쉽게 식별할 수 있다.
- `Librosa`는 Python에서 많이 쓰는 음성 파일 분석 패키지로 음성 파일을 Mel-Spectrogram으로 **시각화**할 수 있다.

### 3.3 데이터 전처리
#### 3.3.1 Hierarchical Clustering

&nbsp;&nbsp;반려동물은 복잡하고 다양한 감정들을 표현할 수 있다. 하지만 사람과는 달리 말을 통해 의사소통을 못하기 때문에 예를 들면 같은 짓는 소리여도 기뻐서 짓는 건지 화가 나서 짓는 건지 소리만 듣고 구분하기 어렵다.  
  
&nbsp;&nbsp;따라서 데이터들에서 서로 가까운 2개의 점을 찾고 그 점을 합쳐 하나로 만들어주고, 이 결과로 새로운 상위점을 만드는 과정을 반복하는 `Hierarchical Clustering(계층적 군집화)`을 이용하여 군집의 유사성에 따라 분류했다.  
  
&nbsp;&nbsp;Hierarchical Clustering은 트리 모형을 이용해 개별 개체들을 **순차적**, **계층적**으로 유사한 개체 내지 그룹과 통합하여 군집화를 수행하는 알고리즘이다. `K-means`와 달리 군집 수를 사전에 정의하지 않아도 학습을 수행할 수 있다. 개체들이 결합되는 순서를 아래 그림과 같은 트리형태의 구조인 `덴드로그램`으로 나타낼 수 있기 때문이다.

![dendrogram](/assets/img/ai/project/dendrogram.png)

#### 3.3.2 Data Augmentation

&nbsp;&nbsp;`딥러닝`의 고질적인 문제는 여러가지 있는데 그 중 대표적인 문제가 `Overfitting(과잉적합)`이다. Overfitting을 해결하기 위한 고전적인 방법으로는 _Regularizaion_, _Normalization_ 인 모델링 수정이다. 하지만 이러한 방법은 **편향 학습 방향**을 조급 줄이는 정도이다. 즉, 결과적으로 overfitting을 완벽히 해결하는 기술이 되진 않는다.  
  
&nbsp;&nbsp;이미지 데이터에서 `Augumentation`은 _알고리즘_ 적인 해결방법이 아니라, _공학적 접근_ 을 통한 추론을 위한 전처리 기술 중 하나이다. 원본 이미지에 인위적인 변화를 줘서 이미지를 충분히 학습에 활용될 수 있는 데이터로 만드는 기술이다.  
  
&nbsp;&nbsp;Data Augmentation은 기존의 데이터의 정보량을 **보존**한 상태로 **노이즈**를 주는 방식이다. 단지 정보량에 약간의 변화를 주는 것으로, 딥러닝으로 분석된 데이터의 강력하게 표현되는 고유의 특징을 느슨하게 만드는 것이다. 이는 결과적으로 overfitting을 막아줄 수 있고 예측 범위를 약간 넓혀줄 수 있다.

![augumentation](/assets/img/ai/project/augmentation.png)

&nbsp;&nbsp;위의 그림을 보면 좌측 상단의 이미지가 원본 이미지이고 나머지가 _뒤틀림_, _뒤집기_ 의 방법으로 **증강**한 Mel-Spectrogram의 이미지이다.

### 3.4 Data Training

- 학습에 맞게 **전처리**된 데이터를 불러와 Attention이 적용된 Mobile-Net 모델로 학습을 시킨다. 또한 어떤 Convolution Block 위치에 Attention을 적용해야 더 좋은 학습을 할 수 있는지 알기 위하여 Attention을 다르게 적용하여 결과를 확인했다.

> 모델에 적용한 Self-Attention 기법인 `CBAM`과 Mobile, Embedded System에서 사용하는 CNN 모델인 `Mobile-Net`의 내용은 '딥러닝 기반 반려견 울음소리 감지와 감정 및 상태 분석(1)'에 명시되어 있다.

- 먼저, 이번 연구에 사용한 `Convolution Block Attention Module`이다.

```python
"""
    기존의 Self Attention을 경량화한 CONVOLUTION BLOCK ATTENTION MODULE
    @FUNCTION se_block : Squeeze and Excitation Block
    @FUNCTION cbam_block : Convolution Block Attetntion Module
    @FUNCTION channel_attention : Channel Attention
    @FUNCITON Spatial_attention : Spation_attention
"""
import numpy as np
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import GlobalAvgPool2D, GlobalMaxPool2D
from tensorflow.keras.layers import Reshape, Dense, Permute, Lambda
from tensorflow.keras.layers import Add, Activation
from tensorflow.keras import backend as K
from keras.activations import sigmoid
from tensorflow.keras import layers

"""
    Squeeze-and-Excitation(SE) Block
    @brief : 채널간의 관계를 재종정 시켜줌
    @param input_feature : tensor
"""
def se_block(input_feature, ratio=8):
    
    se_feature = GlobalAvgPool2D()(input_feature)
    channel = input_feature._shape[-1]
    
    se_feature = Reshape((1, 1, channel))(se_feature)
    se_feature = Dense(channel // ratio,
                       activation='relu',
                       kernel_initializer='he_normal',
                       use_bias=True,
                       bias_initializer='zeros')(se_feature)
    
    se_feature = Dense(channel,
                       activation='sigmoid',
                       kernel_initializer='he_normal',
                       use_bias=True,
                       bias_initializer='zeros')(se_feature)
    
    se_feature = layers.multiply([input_feature, se_feature])
    
    return se_feature

"""
    CBAM_BLOCK
    @brief : Convolution Block Attention Module
    @param cbam_feature : input tensor
    @param ratio(int) : channel reduce ratio
    @return cbam_feature : dynamic feature selection
"""
def cbam_block(cbam_feature, ratio=8):
    
    cbam_feature = channel_attention(cbam_feature, ratio)
    cbam_feature = spatial_attention(cbam_feature)
    
    return cbam_feature

"""
    Channel Attention
    @brief : Channel Attention, average pool과 max pool을 사용(파라미터 양을 줄일 수 있음)
            두 가지 pooled feature는 같은 의미를 공유하는 값이기 때문에 하나의 공유된 MLP를 사용
    @param input_feature = input_tensor
    @return cbam_feature
"""
def channel_attention(input_feature, ratio=8):
    
    # 채널을 먼저 적용
    channel = input_feature._shape[-1]
    
    shared_layer_one = Dense(channel//ratio,
                             activation='relu',
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')
    
    shared_layer_two = Dense(channel,
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')
    
    # average pool과 max pool 두 가지를 결합하여 사용
    avg_pool = GlobalAvgPool2D()(input_feature)
    avg_pool = Reshape((1, 1, channel))(avg_pool)
    avg_pool = shared_layer_one(avg_pool)
    avg_pool = shared_layer_two(avg_pool)
    max_pool = GlobalMaxPool2D()(input_feature)
    max_pool = Reshape((1, 1, channel))(max_pool)
    max_pool = shared_layer_one(max_pool)
    max_pool = shared_layer_two(max_pool)
    
    cbam_feature = Add()([avg_pool, max_pool])
    # 가장 중요한 feature를 찾는 것이 목적이 아니기 때문에 mutually exclusive한
    # softmax 대신 sigmoid를 사용
    cbam_feature = Activation('sigmoid')(cbam_feature)
    cbam_feature = layers.multiply([avg_pool, max_pool])
    return cbam_feature

"""
    Spatial Attention
    @brief : 2차원의 spatial attention, single convolution을 사용하여 특징이 보이는 
            channel을 만듬, 정보가 어디에 있는지 중점을 둠
    @param ipnut_feature : input_tensor(Channel-refined feature)
"""
def spatial_attention(input_feature, kernel_size=7):
    
    cbam_feature = input_feature
    
    avg_pool = Lambda(lambda x : K.mean(x, axis=3, keepdims=True))(cbam_feature)
    max_pool = Lambda(lambda x : K.max(x, axis=3, keepdims=True))(cbam_feature)
    concat = layers.concatenate([avg_pool, max_pool])
    cbam_feature = Conv2D(filters=1,
                          kernel_size=kernel_size,
                          strides=1,
                          padding='same',
                          activation='sigmoid',
                          kernel_initializer='he_normal',
                          use_bias=False)(concat)
    
    cbam_feature = Conv2D(filters=1,
                          kernel_size=kernel_size,
                          strides=1,
                          padding='same',
                          activation='sigmoid',
                          kernel_initializer='he_normal')(concat)
    
    return layers.multiply([input_feature, cbam_feature])
```

- 다음으로 `CBAM`을 적용한 `Mobile-Net`이다.

```python
"""
    ATTENTION을 적용한 MOBILE NET
    @FUNCTION load_data : pickle 데이터를 로딩하는 함수
    @FUNCTION Mobile_net : 모바일 넷 모델 함수
    @FUNCITON predict : 모델 예측하는 함수
"""
import pickle
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import keras
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose
from tensorflow.keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization
from tensorflow.keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU, PReLU
from attention_module import cbam_block

"""
    데이터 로드
    @brief : load Inputs and Targets from pickle data 
    @param data_path(str) : path to pickle file containing data
    @return X(ndarray) : Inputs
    @return y(ndarray) : Targets
"""
def load_data():
    X = pickle.load(open("X_final2.pickle", "rb"))
    y = pickle.load(open("y_final2.pickle", "rb"))
    
    X = X/225.0
    
    return X, y

"""
    mobile net 구현
    @brief : Mobile net with Convolution Block Attention Module(CBAM)
            I used cbam at last of convolution when I used it at every conv block,
            the result was worse.
    @return model : Mobile Net Model
"""
def mobile_net(input_shape):
    
    def mobile_net_block(x, f, s=1):
        x = DepthwiseConv2D(3, strides=s, padding='same')(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        
        x = Conv2D(f, 1, strides=1, padding='same')(x)  
        x = BatchNormalization()(x)
        x = ReLU()(x)
        
        return x
    
    input = Input(input_shape)
    
    x = Conv2D(32, 3, strides=2, padding='same')(input)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    
    x = mobile_net_block(x, 64)
    x = mobile_net_block(x, 128, 2)
    x = mobile_net_block(x, 128)
    
    x = mobile_net_block(x, 256, 2)
    x = mobile_net_block(x, 256)
    
    x = mobile_net_block(x, 512, 2)
    for _ in range(5):
        x = mobile_net_block(x, 512)
        
    x = mobile_net_block(x, 1024, 2)
    x = mobile_net_block(x, 1024)
    x = cbam_block(x)
    
    x = GlobalAvgPool2D()(x)
    
    output = Dense(3, activation='softmax')(x)
    
    model = Model(input, output)
    return model

"""
    학습된 모델로 예측
    @brief : predict data from trained mobile net model
    @param model : Trained classifier
    @param X : Input data
    @param y(int): Target
"""
def predict(model, X, y):
    # sample의 입력 데이터에 차원 추가
    X = X[np.newaxis, ...]
    
    prediction = model.predict(X)
    
    # argmax를 사용해서 index의 최대 값을 얻음
    predicted_index = np.argmax(prediction, axis=1)
    
    print("Target: {}, Predicted label: {}".format(y, predicted_index))

# 메인 함수
if __name__ == "__main__":
    
    # load data and split to X_train and y_train
    X_train, y_train = load_data()
    
    # create network
    K.clear_session()
    input_shape = (X_train.shape[1], X_train.shape[2], 3)
    model= mobile_net(input_shape)
    
    # compile model
    optimiser = keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=optimiser,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    
    model.summary()
    
    # train model
    hist = model.fit(X_train, y_train, validation_split=0.1, batch_size=32, epochs=30)
    
    fig, loss_ax = plt.subplots()
    
    acc_ax = loss_ax.twinx()
    
    loss_ax.plot(hist.history['loss'], 'y', label='train loss')
    loss_ax.plot(hist.history['val_loss'], label='validation loss')
    
    acc_ax.plot(hist.history['acc'], 'b', label='train_acc')
    acc_ax.plot(hist.history['val_acc'], 'g', label='validation_acc')
    
    loss_ax.set_xlabel('epoch')
    loss_ax.set_ylabel('loss')
    acc_ax.set_ylabel('accuracy')
    
    loss_ax.legend(loc='upper left')
    loss_ax.legend(loc='lower left')
    
    # evalute model
    test_loss, test_acc = model.evaluate(X_train, y_train, verbose=2)
    print('\nTest accuracy:', test_acc)
    
    X_to_predict = X_train[100]
    y_to_predict = y_train[100]
    
    # predict sample
    predict(model, X_to_predict, y_to_predict)
    
    model.save('path where your model save')
```