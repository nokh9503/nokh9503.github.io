---
title: 딥러닝 기반 반려견 울음소리 감지와 감정 및 상태 분석(1)
author: nokh9503
date: 2020-12-13 00:00:00 +0800
categories: [AI, Deep Learning]
tags: [AI, Deep Learning]
---

# Analysis of Emotional Conditions through the Barking of Dogs Based on Deep Learning

이 주제는 대학교 졸업 논문으로 쓴 주제이다. 교수님들과 같이 연구할 수 있는 다양한 주제들이 있었지만 나는 산업에서 다룰 수 있는 주제를 하기를 원해서 스타트업과 같이 진행을 했다. 비록, 초창기 스타트업이라 현업께서 굉장히 바쁘시고 다른 프로젝트도 있어서 많이 도와주시진 못했지만 내가 무엇인가를 기획하고 설계할 수 있었음에 굉장히 뜻 깊은 경험이라고 생각한다.

## 요약

&nbsp;&nbsp;현재 반려동물을 양육하는 인구가 많아지면서 관련 상품의 수요도 증가하고 있다. 그중 반려견의 언어를 번역해주는 번역기가 있지만, 신뢰성의 부족과 실시간으로 번역이 불가능하다는 단점이 있다. 이에 본 연구에서는 인공지능의 기법을 이용한 소리와 관련된 감정 구분 기법을 연구하여 반려견의 울음소리에 따른 감정 및 상태 구분기를 만들어 학습시키고 학습된 모델을 구현한다. 본 연구를 통해 견주와 반려견과의 유대감을 증진시키고, 나아가 인공지능을 이용한 서비스 분야에 대한 연구에 의의를 둔다.

## 1. 서론

 &nbsp;&nbsp;한국농촌경제연구원 등에 따르면 국내 반려동물 양육가구는 전체 가구의 26.4%, 인구로는 1500 만 명에 달하는 것으로 추산된다. 이에 반려동물 관련 상품 및 서비스가 출시되면서 반려동물 시장의 규모는 증가하고 있으며, 향후 펫 서비스업이 반려동물 산업에서 중요한 부분을 차지할 것으로 예상한다.  
  
&nbsp;&nbsp;최근 강력한 병렬 처리 성능을 제공하는 GPU(Graphics Processing Unit)의 도입과 빅데이터의 발전으로 인공지능이 급속도로 발전하고 있다. 인공지능의 연구가 진행됨에 따라 감정을 인식하고 서비스를 제공하는 기술에 관심이 높아지고 있으며, 연구가 진행되고 있으나 대부분 영상과 음성을 결합하여 사용하거나 영상만을 사용한 경우가 많다.  
  
&nbsp;&nbsp;본 연구에서는 딥러닝을 이용한 반려견 울음소리를 인식하고 울음소리에 따른 감정 및 상태를 실시간으로 분석하는 방안을 제안한다.

## 2. 관련연구
### 2.1 Spectrogram

&nbsp;&nbsp;`Spectrogram`은 시간이 변화하면서 소리나 다른 신호의 강도나 세기가 변하는 것을 다른 주파수에 따라 시각적으로 표현한 것이다. Spectrogram의 가로축은 시간을 나타내며 세로축은 주파수를 나타내며 소리나 신호의 강도나 세기가 변화함에 따라 spectrogram에서 표현하는 색도 달라진다.  

![spectrogram](/assets/img/ai/project/spectrogram.png)

&nbsp;&nbsp;특정 울음소리를 발화함에 따라 위의 그림과 같이 다르게 표현되는 spectrogram을 이용하여 음성 데이터에서의 감정을 분류하고 인식한다.  
  
&nbsp;&nbsp;각각의 음성 데이터로부터 spectrogram을 추출 후 시간 축을 일정한 간격으로 나누어 정형화된 형태를 입력 값으로 사용한다.

### 2.2 Unsupervised Learning

&nbsp;&nbsp;`비지도 학습(Unsupervied Learning)`은 머신러닝의 일종으로 데이터가 어떻게 구성되었는지 알아내는 문제의 범주에 속한다. 컴퓨터에게 label이 무엇인지 알려주면서 학습시키는 `지도 학습(Supervised Learning)`과 처벌과 보상이 있는 `강화 학습(Reinforcement Learning)`과 달리 입력 값에 대한 label이 주어지지 않고 비슷한 데이터를 군집화를 통해 분류한다.

![cluster algorithm](/assets/img/ai/project/cluster_algorithm.png)

&nbsp;&nbsp;반려견은 짓는 소리 외에 우는 소리를 낸다. Whimpeling과 같이 높은 소리로 울거나 Howling과 같이 낮게 소리를 내는 경우가 있다. 개가 소리를 내는 것은 인간이 말을 하는 것과도 같다. 인간처럼 말을 통해 의사소통을 못하지만 무언가를 전달하거나 드러내기 위해 소리로 표현하는 것이다.  
  
&nbsp;&nbsp;하지만 같은 높은 소리거나 낮은 소리여도 다른 의미를 나타내기도 한다. 이 경우에는 울음소리 뿐만 아니라 귀나 꼬리의 움직임, 표정까지 보고 다른 의미로 해석이 되어야하기 때문에 소리만 듣고 정확하게 판단하기 힘들다.
    
&nbsp;&nbsp;본 연구에서는 label을 나누어서 학습을 시키는 모델을 통해서는 위의 문제를 개선하고자 비지도 학습을 통해 반려견의 울음소리를 통해 상태 및 감정을 분류하고자 한다.

### 2.3 Mobile-Net

&nbsp;&nbsp;`실시간 객체 검출`을 하기 위한 머신 비전 기법으로는 전통적인 필터 기반의 _접근법 필터_ 와 _SVM_ 등의 기계학습을 결합한 접근법, 그리고 _CNN_ 등을 이용한 딥러닝 접근법이 있다.  
**특징 정보를 효과적으로 추출**하기 위해 CNN의 layer를 더 깊게 쌓아 높이는 방식으로 성능을 개선시키려는 연구가 활발히 진행되고 있다.

![org_convolution](/assets/img/ai/project/org_convolution.png)

&nbsp;&nbsp;그러나 위의 그림과 같이 CNN에서 layer가 깊어질수록 파라미터 개수와 연산량이 기하급수적으로 증가하는 문제가 발생하게 된다. 이러한 문제는 mobile device, embedded system과 같은 제한된 리소스를 가지는 환경에서 더 심각한 문제가 될 수 있다.  
  
&nbsp;&nbsp;고성능의 device가 아니라 자동차, 드론, mobile가 같은 환경에서는 CPU를 하나 정도 가지고 있는 경우도 많고 **GPU가 없을 수도 있으며**, **메모리도 부족하다**. 또한 배터리를 사용하는 경우가 많기 때문에 성능을 높이는 것은 **전력 소모가 늘어나기 때문**에 어렵다.

![depthwise seperable convolution](/assets/img/ai/project/depthwise_seperable_convolution.png)

&nbsp;&nbsp;`Mobile Net`에서는 그림과 같이 **Depthwise Seperable Convolution** 연산을 사용하여 각 채널 별로 convolution 연산을 실행하고 그 결과에 1x1 convolution 연산을 취하는 것이다. 기존의 convolution이 모든 채널과 지역 정보를 고려해서 하나의 _feature map_ 을 만들었다면, depthwise convolution은 각 채널 별로 _feature map_ 을 하나씩 만들고 그 다음 1x1 convolu*tion 연산을 수해하여 출력되는 feature map 수를 조정한다. 
   
&nbsp;&nbsp;따라서 **컴퓨터 성능**이 제한되거나 **배터리 퍼포먼스**가 중요한 곳에서 사용될 목적으로 설계된 가벼운 CNN 구조인 `Mobile Net`으로 이러한 문제점들을 개선한다.

### 2.4 Convolution Block Attention Module (CBAM)

![attention module](/assets/img/ai/project/attention_module.png)

&nbsp;&nbsp;`Attention` 매커니즘은 기계번역과 같은 `자연어 처리(NLP)` 분야에서 주로 사용되었지만 최근 Image Classification, Object Detection, Segmentation, Pose Estimation 등 다양한 컴퓨터 비전 분야에서도 활발하게 사용되고 있다.  
  
&nbsp;&nbsp;CNN에서 특징의 채널은 특징에 대한 다양한 정보를 담고 있으며 high-level의 채널 정보는 클래스와 더욱 밀접한 정보를 담고 있다. 따라서 특징의 채널들 간의 **dependency**를 분석하고 더 의미 있는 정보를 담고 있는 채널에 더 높은 **가중치**를 부여하는 `channel attention`을 통해 모델의 성능을 높이려는 연구가 다양한 분야에서 활발히 활용되고 있다.  

&nbsp;&nbsp;CNN에서 성능 향상을 위한 `self attenion`모듈은 `CBAM`을 사용한다. CBAM은 모듈의 **pooling**, **spatial**과 **channel attention**의 결합 방식을 적용한 attention 모듈로 channel을 먼저 적용하여 기존의 self-attention 기법 보다 더 나은 성능을 보이고 있다.

![CBAM](/assets/img/ai/project/CBAM.png)

&nbsp;&nbsp;그림에 보이는 것과 같이 각 attention의 구성을 보면 다음과 같다. `Channel Attention`의 경우 **average pool**, **max pool** 두 가지를 결합하여 사용한다. 3차원의 feature map의 max pool과 avg pool 값은 global한 attentioning에서 유의미한 static으로 사용될 수 있다고 볼 수 있다. 또한 두 가지 pooling된 feature는 같은 의미를 공유하는 값이기 때문에 **하나의 공유된 MLP**를 사용할 수 있으며, 파라미터 양을 줄일 수 있다. `Spatial Attention` 역시 대칭적으로 구성되어, 하나의 convolution으로 spatial attention을 계산한다.  
  
&nbsp;&nbsp;따라서 일반적으로 convolution block으로 구성되어 있는 CNN들은 **경량화**된 CBAM으로 모든 convolution block 마지막에 쉽게 추가될 수 있다.